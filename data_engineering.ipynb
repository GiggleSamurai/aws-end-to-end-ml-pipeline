{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e4cb51",
   "metadata": {},
   "source": [
    "# AWS End-to-End ML Pipeline Demo\n",
    "\n",
    "# Data Lake Formation & Data Imputation (Part I)\n",
    "### Author: Louis Wong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f026b65",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "We are creating an environment using the AWS Glue Docker image and installing the packages we need. We will be using Boto3 for AWS APIs, PySpark/Delta-Spark for ETL, SageMaker for machine learning, and Matplotlib to visualize the results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "40e50302",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!docker run -dit \\\n",
    "  --platform linux/amd64 \\\n",
    "  --name glue-notebook \\\n",
    "  -p 8888:8888 -p 4040:4040 \\\n",
    "  -v /Users/louisw/Documents/aws-end-to-end-ml-pipeline:/home/hadoop/workspace \\\n",
    "  public.ecr.aws/glue/aws-glue-libs:5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b00fdeb0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!docker exec -it glue-notebook pip3 install --user notebook jupyterlab\n",
    "\n",
    "!docker exec -d glue-notebook \\\n",
    "  /home/hadoop/.local/bin/jupyter lab \\\n",
    "  --ip=0.0.0.0 \\\n",
    "  --port=8888 \\\n",
    "  --no-browser \\\n",
    "  --ServerApp.token='' \\\n",
    "  --ServerApp.root_dir=/home/hadoop/workspace"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c7fe96b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip install sagemaker pandas matplotlib minio dotenv sklearn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0b14d9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinIO user: minio \n",
      "MinIO password: ********\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "minio_user = os.getenv(\"MINIO_ROOT_USER\")\n",
    "minio_pass = os.getenv(\"MINIO_ROOT_PASSWORD\")\n",
    "\n",
    "print(f\"MinIO user: {minio_user} \\nMinIO password: {'*' * len(minio_pass)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b322e20",
   "metadata": {},
   "source": [
    "Let’s store the credentials in environment variables to avoid hardcoding them into the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfdb5995",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .gitignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gitignore\n",
    "__pycache__/\n",
    ".ipynb_checkpoints/\n",
    ".env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd2264",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "We are creating a .gitignore file to ignore Python cache files, Jupyter Notebook checkpoints, and environment variable files."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2f1696b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!docker run -d --name minio_local \\\n",
    "    -p 9000:9000 -p 9090:9090 \\\n",
    "    -e MINIO_ROOT_USER=$MINIO_ROOT_USER \\\n",
    "    -e MINIO_ROOT_PASSWORD=$MINIO_ROOT_PASSWORD \\\n",
    "    quay.io/minio/minio server /data --console-address \":9090\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f900ea1",
   "metadata": {},
   "source": [
    "# ETL (Extract, Transform, Load)\n",
    "Now that we have our environment set up, let’s move on to ETL to process the data. We are going to extract, transform, and load the data into the simulated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f954ed8f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'raw-data' created.\n",
      "'silver-data' created.\n",
      "'gold-data' created.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Establish connection to S3\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://host.docker.internal:9000\", # Internal Docker network to access\n",
    "    aws_access_key_id=minio_user,\n",
    "    aws_secret_access_key=minio_pass,\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# Let's create a reusable function to create bucket if not exists\n",
    "def check_and_create_bucket(s3, bucket):\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket)\n",
    "        print(f\"'{bucket}' already exists.\")\n",
    "    except botocore.exceptions.ClientError:\n",
    "        s3.create_bucket(Bucket=bucket)\n",
    "        print(f\"'{bucket}' created.\")\n",
    "\n",
    "# Create a medallion architecture organization with three buckets\n",
    "buckets = [\"raw-data\", \"silver-data\", \"gold-data\"]\n",
    "\n",
    "for bucket in buckets:\n",
    "    check_and_create_bucket(s3, bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c1b64",
   "metadata": {},
   "source": [
    "Loading our dataset, for more information about the dataset, please visit: https://gist.github.com/aishwarya8615/d2107f828d3f904839cbcb7eaa85bd04\n",
    "\n",
    "This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, and various diseases and smoking status. A subset of the original train data is taken using the filtering method for Machine Learning and Data Visualization purposes.\n",
    "\n",
    "About the Data: Each row in the data provides relavant information about a person , for instance; age, gender,smoking status, occurance of stroke in addition to other information Unknown in Smoking status means the information is unavailable. N/A in other input fields imply that it is not applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4afc6b84",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5109 entries, 0 to 5108\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Admission Date     5109 non-null   object \n",
      " 1   Full Name          5109 non-null   object \n",
      " 2   Phone Number       5109 non-null   object \n",
      " 3   Address            5109 non-null   object \n",
      " 4   Gender             5109 non-null   object \n",
      " 5   Age                5109 non-null   float64\n",
      " 6   Hypertension       5109 non-null   int64  \n",
      " 7   Heart Disease      5109 non-null   int64  \n",
      " 8   Ever Married       5109 non-null   object \n",
      " 9   Work Type          5109 non-null   object \n",
      " 10  Residence Type     5109 non-null   object \n",
      " 11  Avg Glucose Level  5109 non-null   float64\n",
      " 12  BMI                4908 non-null   float64\n",
      " 13  Smoking Status     5109 non-null   object \n",
      " 14  Stroke             5109 non-null   int64  \n",
      "dtypes: float64(3), int64(3), object(9)\n",
      "memory usage: 598.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Heart Disease</th>\n",
       "      <th>Avg Glucose Level</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5109.000000</td>\n",
       "      <td>5109.000000</td>\n",
       "      <td>5109.000000</td>\n",
       "      <td>5109.000000</td>\n",
       "      <td>4908.00000</td>\n",
       "      <td>5109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43.229986</td>\n",
       "      <td>0.097475</td>\n",
       "      <td>0.054022</td>\n",
       "      <td>106.140399</td>\n",
       "      <td>28.89456</td>\n",
       "      <td>0.048738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22.613575</td>\n",
       "      <td>0.296633</td>\n",
       "      <td>0.226084</td>\n",
       "      <td>45.285004</td>\n",
       "      <td>7.85432</td>\n",
       "      <td>0.215340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.120000</td>\n",
       "      <td>10.30000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.240000</td>\n",
       "      <td>23.50000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.880000</td>\n",
       "      <td>28.10000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>114.090000</td>\n",
       "      <td>33.10000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>271.740000</td>\n",
       "      <td>97.60000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age  Hypertension  Heart Disease  Avg Glucose Level  \\\n",
       "count  5109.000000   5109.000000    5109.000000        5109.000000   \n",
       "mean     43.229986      0.097475       0.054022         106.140399   \n",
       "std      22.613575      0.296633       0.226084          45.285004   \n",
       "min       0.080000      0.000000       0.000000          55.120000   \n",
       "25%      25.000000      0.000000       0.000000          77.240000   \n",
       "50%      45.000000      0.000000       0.000000          91.880000   \n",
       "75%      61.000000      0.000000       0.000000         114.090000   \n",
       "max      82.000000      1.000000       1.000000         271.740000   \n",
       "\n",
       "              BMI       Stroke  \n",
       "count  4908.00000  5109.000000  \n",
       "mean     28.89456     0.048738  \n",
       "std       7.85432     0.215340  \n",
       "min      10.30000     0.000000  \n",
       "25%      23.50000     0.000000  \n",
       "50%      28.10000     0.000000  \n",
       "75%      33.10000     0.000000  \n",
       "max      97.60000     1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset locally\n",
    "df = pd.read_csv(\"./data/healthcare-dataset-stroke-data.csv\")\n",
    "\n",
    "df.info()\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac2594",
   "metadata": {},
   "source": [
    "Before we load the dataset into S3, let's drop all the sensitive PII (Personally Identifiable Information), such as Name, Phone, Address, Billing, SSN, and Credit Card, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47089f",
   "metadata": {},
   "source": [
    "For this case we will drop the patient name, phone number and address. If there are patient DOB (date of birth), we shall convert to age (round by year) and drop the actual date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1502da97",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1868E138AA20AD1B',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'content-length': '0',\n",
       "   'etag': '\"a1fd5b75152c7874e1e58f267b3e13ca\"',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-checksum-crc32': 'XpKlPw==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '1868E138AA20AD1B',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '2143',\n",
       "   'x-ratelimit-remaining': '2143',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Fri, 26 Sep 2025 16:14:48 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"a1fd5b75152c7874e1e58f267b3e13ca\"',\n",
       " 'ChecksumCRC32': 'XpKlPw==',\n",
       " 'ChecksumType': 'FULL_OBJECT'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop PII columns\n",
    "df = df.drop(columns=[\"Full Name\", \"Phone Number\", \"Address\"])\n",
    "\n",
    "# Load to S3\n",
    "csv_buffer = df.to_csv(index=False)\n",
    "s3.put_object(Bucket=\"raw-data\", Key=\"healthcare-dataset-stroke-data.csv\", Body=csv_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e2cff",
   "metadata": {},
   "source": [
    "In real-world big data with millions of records, it’s more efficient to use distributed frameworks like PySpark. Converting the dataset to Delta Lake improves query speed, storage reliability, ACID compliance, and tool integration, while also enabling scalable analytics and time travel for audits and reproducibility.\n",
    "\n",
    "## PySpark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c929f1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-69348da4-1990-43d3-a976-dc815474233d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 450ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-69348da4-1990-43d3-a976-dc815474233d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/13ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|Admission Date|Gender| Age|Hypertension|Heart Disease|Ever Married|    Work Type|Residence Type|Avg Glucose Level| BMI| Smoking Status|Stroke|\n",
      "+--------------+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|    2025-03-03|  Male|67.0|           0|            1|         Yes|      Private|         Urban|           228.69|36.6|formerly smoked|     1|\n",
      "|    2025-04-03|Female|61.0|           0|            0|         Yes|Self-employed|         Rural|           202.21|NULL|   never smoked|     1|\n",
      "|    2025-06-27|  Male|80.0|           0|            1|         Yes|      Private|         Rural|           105.92|32.5|   never smoked|     1|\n",
      "|    2025-06-19|Female|49.0|           0|            0|         Yes|      Private|         Urban|           171.23|34.4|         smokes|     1|\n",
      "|    2025-01-03|Female|79.0|           1|            0|         Yes|Self-employed|         Rural|           174.12|24.0|   never smoked|     1|\n",
      "+--------------+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "s3_endpoint = \"http://host.docker.internal:9000\"\n",
    "\n",
    "# Create SparkSession with MinIO S3 config\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"SparkJob\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", s3_endpoint)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", minio_user)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", minio_pass)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") # MinIO does not use SSL by default\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# MinIO requires setting credentials provider explicitly\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "# Load data from S3 to Spark DataFrame\n",
    "bucket = \"raw-data\"\n",
    "key = \"healthcare-dataset-stroke-data.csv\"\n",
    "s3_path = f\"s3a://{bucket}/{key}\"\n",
    "\n",
    "df = spark.read.csv(s3_path, header=True, inferSchema=True)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9a6245",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+---+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|admission_date|gender|age|hypertension|heart_disease|ever_married|    work_type|residence_type|avg_glucose_level| bmi| smoking_status|stroke|\n",
      "+--------------+------+---+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|    2025-03-03|  Male| 67|           0|            1|           1|      Private|         Urban|           228.69|36.6|formerly smoked|     1|\n",
      "|    2025-04-03|Female| 61|           0|            0|           1|Self-employed|         Rural|           202.21|NULL|   never smoked|     1|\n",
      "|    2025-06-27|  Male| 80|           0|            1|           1|      Private|         Rural|           105.92|32.5|   never smoked|     1|\n",
      "|    2025-06-19|Female| 49|           0|            0|           1|      Private|         Urban|           171.23|34.4|         smokes|     1|\n",
      "|    2025-01-03|Female| 79|           1|            0|           1|Self-employed|         Rural|           174.12|24.0|   never smoked|     1|\n",
      "+--------------+------+---+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Clean up column names (lowercase, replace spaces with underscores)\n",
    "for c in df.columns:\n",
    "    \n",
    "    df = df.withColumnRenamed(c, c.strip().lower().replace(\" \", \"_\"))\n",
    "\n",
    "# Fix data types where needed\n",
    "df = (df\n",
    "    .withColumn(\"admission_date\", F.to_date(\"admission_date\", \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"age\", F.col(\"age\").cast(\"integer\"))\n",
    "    .withColumn(\"bmi\", F.col(\"bmi\").cast(\"float\"))\n",
    "    .withColumn(\"ever_married\", F.when(F.col(\"ever_married\") == \"Yes\", 1).otherwise(0))\n",
    ")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a24d0a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admission_date: 0 nulls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender: 0 nulls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 0 nulls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypertension: 0 nulls\n",
      "heart_disease: 0 nulls\n",
      "ever_married: 0 nulls\n",
      "work_type: 0 nulls\n",
      "residence_type: 0 nulls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_glucose_level: 0 nulls\n",
      "bmi: 201 nulls\n",
      "smoking_status: 0 nulls\n",
      "stroke: 0 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Check for nulls in each column\n",
    "for c in df.columns:\n",
    "    null_count = df.filter(col(c).isNull()).count()\n",
    "    print(f\"{c}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf57383",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|work_type    |count|\n",
      "+-------------+-----+\n",
      "|Private      |2924 |\n",
      "|Self-employed|819  |\n",
      "|children     |687  |\n",
      "|Govt_job     |657  |\n",
      "|Never_worked |22   |\n",
      "+-------------+-----+\n",
      "\n",
      "+--------------+-----+\n",
      "|residence_type|count|\n",
      "+--------------+-----+\n",
      "|Urban         |2596 |\n",
      "|Rural         |2513 |\n",
      "+--------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|smoking_status |count|\n",
      "+---------------+-----+\n",
      "|never smoked   |1892 |\n",
      "|Unknown        |1544 |\n",
      "|formerly smoked|884  |\n",
      "|smokes         |789  |\n",
      "+---------------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|Female|2994 |\n",
      "|Male  |2115 |\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check distinct values in categorical columns\n",
    "for col_name in ['work_type','residence_type','smoking_status','gender']:\n",
    "    df.groupBy(col_name).count().orderBy(\"count\", ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93dae4",
   "metadata": {},
   "source": [
    "## Impute missing BMI Values\n",
    "We will impute missing BMI Values with the using MICE (Multiple Imputation by Chained Equations) on gender, age, and average glucose level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875cd9ff",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/26 16:15:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+---+------------+-------------+------------+-------------+--------------+-----------------+---------------+------+-----+\n",
      "|admission_date|gender|age|hypertension|heart_disease|ever_married|    work_type|residence_type|avg_glucose_level| smoking_status|stroke|  bmi|\n",
      "+--------------+------+---+------------+-------------+------------+-------------+--------------+-----------------+---------------+------+-----+\n",
      "|    2025-03-03|  Male| 67|           0|            1|           1|      Private|         Urban|           228.69|formerly smoked|     1| 36.6|\n",
      "|    2025-04-03|Female| 61|           0|            0|           1|Self-employed|         Rural|           202.21|   never smoked|     1|32.62|\n",
      "|    2025-06-27|  Male| 80|           0|            1|           1|      Private|         Rural|           105.92|   never smoked|     1| 32.5|\n",
      "|    2025-06-19|Female| 49|           0|            0|           1|      Private|         Urban|           171.23|         smokes|     1| 34.4|\n",
      "|    2025-01-03|Female| 79|           1|            0|           1|Self-employed|         Rural|           174.12|   never smoked|     1| 24.0|\n",
      "+--------------+------+---+------------+-------------+------------+-------------+--------------+-----------------+---------------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Convert to pandas\n",
    "pdf = df.select(\"gender\", \"age\", \"avg_glucose_level\", \"bmi\").toPandas()\n",
    "\n",
    "# Convert categorical to numeric\n",
    "le = LabelEncoder()\n",
    "pdf[\"gender\"] = le.fit_transform(pdf[\"gender\"])\n",
    "\n",
    "# Impute using MICE (Multiple Imputation by Chained Equations) on missing bmi values\n",
    "original_bmi = pdf[\"bmi\"]\n",
    "imputed = IterativeImputer().fit_transform(pdf)\n",
    "imputed_bmi = imputed[:, pdf.columns.get_loc(\"bmi\")]\n",
    "pdf[\"bmi\"] = np.where(original_bmi.isnull(), imputed_bmi, original_bmi).round(2) # Round to 2 decimal places to keep it consistent\n",
    "\n",
    "# Add them back to the original Spark DataFrame using withColumn\n",
    "bmi_values = pdf[\"bmi\"].tolist()\n",
    "df = df.drop(\"bmi\").withColumn(\"bmi\", F.array([lit(v) for v in bmi_values])[F.monotonically_increasing_id()])\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18fadfb1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthcare-dataset-stroke-data/_delta_log/00000000000000000000.crc\n",
      "healthcare-dataset-stroke-data/_delta_log/00000000000000000000.json\n",
      "healthcare-dataset-stroke-data/_delta_log/_commits/\n",
      "healthcare-dataset-stroke-data/admission_year_month=2025-01/part-00000-c96f2681-2503-4be8-a191-0e55983947da.c000.snappy.parquet\n",
      "healthcare-dataset-stroke-data/admission_year_month=2025-02/part-00000-46481111-d62e-4581-8d52-c0885d139ecc.c000.snappy.parquet\n",
      "healthcare-dataset-stroke-data/admission_year_month=2025-03/part-00000-b6a18d42-3f3c-45be-a187-7fe4d08032df.c000.snappy.parquet\n",
      "healthcare-dataset-stroke-data/admission_year_month=2025-04/part-00000-c341953d-4299-4e0f-97bf-3ea20e985980.c000.snappy.parquet\n",
      "healthcare-dataset-stroke-data/admission_year_month=2025-05/part-00000-f8e41e79-e086-4a1c-b610-b17374a1ddd6.c000.snappy.parquet\n",
      "healthcare-dataset-stroke-data/admission_year_month=2025-06/part-00000-45c8b47c-075b-483e-a55f-709685ce53db.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Define the silver layer location\n",
    "silver_bucket = \"silver-data\"\n",
    "silver_key    = \"healthcare-dataset-stroke-data\"\n",
    "silver_path   = f\"s3a://{silver_bucket}/{silver_key}\"\n",
    "\n",
    "# Create a year-month column for partitioning\n",
    "df = df.withColumn(\"admission_year_month\", F.date_format(\"admission_date\", \"yyyy-MM\"))\n",
    "\n",
    "# Write to Delta Lake partitioned by admission_year_month\n",
    "(df.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .partitionBy(\"admission_year_month\")\n",
    "   .save(silver_path)\n",
    ")\n",
    "\n",
    "# Check files in silver bucket\n",
    "response = s3.list_objects_v2(Bucket=silver_bucket, Prefix=silver_key)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9da4d5",
   "metadata": {},
   "source": [
    "In a real-world scenario, if the dataset is too large to load all at once, we can perform imputation in chunks or partitions. Each processed batch can then be written to Delta Lake in append mode before moving on to the next segment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
